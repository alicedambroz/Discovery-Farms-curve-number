{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLQuuh3sTDwoJRZjPQcV8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicedambroz/Discovery-Farms-curve-number/blob/main/Discovery_Farms_RW_Curve_Number.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "debpsSRVT0Nd"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch run for RWS, RWN and MC"
      ],
      "metadata": {
        "id": "lKIiSeZEOofs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import auth, files\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# 1. Authenticate (Standard Colab setup)\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CONFIGURATION: Add your Site Names and Spreadsheet IDs here\n",
        "# ---------------------------------------------------------\n",
        "sites_config = {\n",
        "    'MC1':  '152H5O0QNSPoVYDgHuSD8Tq7C4rWKppXwhh8TSeP1DsY',   # ID\n",
        "    'RW1N': '1wbd9KU4-aVSgwU39b-P8cs9UeREUWwNGzooJYV5GLRk',  # ID\n",
        "    'RW1S': '10kwU-3ZJ1lkK-5dymXfjpI-Zz2tE-1cqnVVSo-Y_SBE' # ID\n",
        "}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# THE PROCESSING FUNCTION\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def process_site_data(site_name, sheet_id):\n",
        "    print(f\"Processing Site: {site_name}...\")\n",
        "\n",
        "    try:\n",
        "        spreadsheet = gc.open_by_key(sheet_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening sheet for {site_name}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- A. Load Data ---\n",
        "    sheet_p = spreadsheet.worksheet(\"Precipitation\")\n",
        "    rows_p = sheet_p.get_all_values()\n",
        "    df_p = pd.DataFrame(rows_p[1:], columns=rows_p[0])\n",
        "\n",
        "    sheet_q = spreadsheet.worksheet(\"Runoff\")\n",
        "    rows_q = sheet_q.get_all_values()\n",
        "    df_q = pd.DataFrame(rows_q[1:], columns=rows_q[0])\n",
        "\n",
        "    sheet_ev = spreadsheet.worksheet(\"Events\")\n",
        "    rows_ev = sheet_ev.get_all_values()\n",
        "    df_events_meta = pd.DataFrame(rows_ev[1:], columns=rows_ev[0])\n",
        "\n",
        "    # --- B. Clean Data Types ---\n",
        "    # Precipitation\n",
        "    df_p['Rainfall_mm'] = pd.to_numeric(df_p['Rainfall_mm'], errors='coerce').fillna(0)\n",
        "    df_p['Date'] = pd.to_datetime(df_p['Date'], format='mixed', dayfirst=False)\n",
        "\n",
        "    # Runoff\n",
        "    df_q['Volume_mm'] = pd.to_numeric(df_q['Volume_mm'], errors='coerce').fillna(0)\n",
        "    df_q['Event'] = pd.to_numeric(df_q['Event'], errors='coerce').fillna(0)\n",
        "    df_q['Date'] = pd.to_datetime(df_q['Date'], format='mixed', dayfirst=False)\n",
        "\n",
        "    # Events Table\n",
        "    df_events_meta['Event'] = pd.to_numeric(df_events_meta['Event'], errors='coerce')\n",
        "    df_events_meta['Start'] = pd.to_datetime(df_events_meta['Start'], format='mixed', dayfirst=False)\n",
        "    df_events_meta['End'] = pd.to_datetime(df_events_meta['End'], format='mixed', dayfirst=False)\n",
        "\n",
        "    # Clean the 'Frozen' column (Ensure it's a string)\n",
        "    # If the column name in your sheet is different (e.g. \"Condition\"), change 'Frozen' below\n",
        "    if 'Frozen' in df_events_meta.columns:\n",
        "        df_events_meta['Frozen'] = df_events_meta['Frozen'].astype(str)\n",
        "    else:\n",
        "        df_events_meta['Frozen'] = 'Unknown' # Fallback if column missing\n",
        "\n",
        "    df_events_meta = df_events_meta[df_events_meta['Event'] > 0].copy()\n",
        "\n",
        "    # --- C. Create Master Timeline ---\n",
        "    min_date = min(df_p['Date'].min(), df_q['Date'].min())\n",
        "    max_date = max(df_p['Date'].max(), df_q['Date'].max())\n",
        "    time_index = pd.date_range(start=min_date, end=max_date, freq='1min')\n",
        "    df_master = pd.DataFrame({'Date': time_index})\n",
        "\n",
        "    # --- D. Merge Data ---\n",
        "    df_master = df_master.merge(df_q[['Date', 'Volume_mm']], on='Date', how='left')\n",
        "    df_master = df_master.merge(df_p[['Date', 'Rainfall_mm']], on='Date', how='left')\n",
        "    df_master['Volume_mm'] = df_master['Volume_mm'].fillna(0)\n",
        "    df_master['Rainfall_mm'] = df_master['Rainfall_mm'].fillna(0)\n",
        "\n",
        "    # --- E. Extract Events ---\n",
        "    site_events_list = []\n",
        "\n",
        "    for _, ev in df_events_meta.iterrows():\n",
        "        event_id = ev['Event']\n",
        "        start_flow = ev['Start']\n",
        "        end_flow = ev['End']\n",
        "        frozen_status = ev['Frozen'] # <--- CAPTURE FROZEN STATUS\n",
        "\n",
        "        if pd.isna(start_flow) or pd.isna(end_flow):\n",
        "            continue\n",
        "\n",
        "        start_plot = start_flow - pd.Timedelta(hours=6)\n",
        "\n",
        "        mask = (df_master['Date'] >= start_plot) & (df_master['Date'] <= end_flow)\n",
        "        df_event_slice = df_master.loc[mask].copy()\n",
        "\n",
        "        df_event_slice['Event'] = event_id\n",
        "        df_event_slice['Site'] = site_name\n",
        "        df_event_slice['Frozen'] = frozen_status # <--- SAVE TO DATA\n",
        "\n",
        "        site_events_list.append(df_event_slice)\n",
        "\n",
        "    if site_events_list:\n",
        "        return pd.concat(site_events_list, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# MAIN EXECUTION\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "all_sites_data = []\n",
        "\n",
        "# Loop through the dictionary defined at the top\n",
        "for site, sheet_id in sites_config.items():\n",
        "    df_site = process_site_data(site, sheet_id)\n",
        "    if not df_site.empty:\n",
        "        all_sites_data.append(df_site)\n",
        "\n",
        "# Concatenate all sites into one big DataFrame\n",
        "if all_sites_data:\n",
        "    df_final = pd.concat(all_sites_data, ignore_index=True)\n",
        "\n",
        "    # Preview\n",
        "    print(\"\\nProcessing Complete!\")\n",
        "    print(df_final.head())\n",
        "    print(f\"Total rows: {len(df_final)}\")\n",
        "\n",
        "    # Save and Download\n",
        "    df_final.to_csv('All_Sites_Events_Plotting.csv', index=False)\n",
        "    files.download('All_Sites_Events_Plotting.csv')\n",
        "else:\n",
        "    print(\"No data processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "AXOpfRM1OuBK",
        "outputId": "ecfed4e1-8717-4796-a79d-cbd0c6614253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Site: MC1...\n",
            "Processing Site: RW1N...\n",
            "Processing Site: RW1S...\n",
            "\n",
            "Processing Complete!\n",
            "                 Date  Volume_mm  Rainfall_mm  Event Site  Frozen\n",
            "0 2017-02-15 07:14:00        0.0          0.0      1  MC1  frozen\n",
            "1 2017-02-15 07:15:00        0.0          0.0      1  MC1  frozen\n",
            "2 2017-02-15 07:16:00        0.0          0.0      1  MC1  frozen\n",
            "3 2017-02-15 07:17:00        0.0          0.0      1  MC1  frozen\n",
            "4 2017-02-15 07:18:00        0.0          0.0      1  MC1  frozen\n",
            "Total rows: 273363\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_05c2c3ea-3045-40e6-ae46-ecb3c56d6bb3\", \"All_Sites_Events_Plotting.csv\", 13747603)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CALCULATE S AND CN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "print(\"Calculating Curve Numbers...\")\n",
        "\n",
        "# 1. Group by Site and Event to get Totals\n",
        "# We also include 'Frozen' in the grouping to keep that column\n",
        "summary = df_final.groupby(['Site', 'Event', 'Frozen']).agg(\n",
        "    Start_Time=('Date', 'min'),\n",
        "    End_Time=('Date', 'max'),\n",
        "    Total_Rainfall_mm=('Rainfall_mm', 'sum'),\n",
        "    Total_Runoff_mm=('Volume_mm', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# 2. Define the Functions\n",
        "def calculate_S(P, Q):\n",
        "    # Standard SCS-CN inversion formula\n",
        "    # S = 5 * (P + 2Q - sqrt(4Q^2 + 5PQ))\n",
        "    # Note: Valid only if P >= Q. If Q > P, it implies physical impossibility for this method (or baseflow issues).\n",
        "\n",
        "    if Q <= 0:\n",
        "        return np.nan # No runoff = Infinite retention (theoretically)\n",
        "    if Q >= P:\n",
        "        return 0 # Runoff >= Rain implies S=0 (saturated/impervious) or data error\n",
        "\n",
        "    term1 = P + (2 * Q)\n",
        "    term2 = np.sqrt((4 * (Q**2)) + (5 * P * Q))\n",
        "    S = 5 * (term1 - term2)\n",
        "    return S\n",
        "\n",
        "def calculate_CN(S):\n",
        "    if pd.isna(S):\n",
        "        return np.nan\n",
        "    # CN = 25400 / (S + 254)\n",
        "    return 25400 / (S + 254)\n",
        "\n",
        "# 3. Apply Calculations Row-by-Row\n",
        "summary['S_mm'] = summary.apply(\n",
        "    lambda row: calculate_S(row['Total_Rainfall_mm'], row['Total_Runoff_mm']), axis=1\n",
        ")\n",
        "\n",
        "summary['CN'] = summary['S_mm'].apply(calculate_CN)\n",
        "\n",
        "# 4. Cleanup and Formatting\n",
        "# Rounding for cleaner output\n",
        "summary['Total_Rainfall_mm'] = summary['Total_Rainfall_mm'].round(4)\n",
        "summary['Total_Runoff_mm'] = summary['Total_Runoff_mm'].round(4)\n",
        "summary['S_mm'] = summary['S_mm'].round(4)\n",
        "summary['CN'] = summary['CN'].round(4)\n",
        "\n",
        "# Sort for readability\n",
        "summary = summary.sort_values(by=['Site', 'Event'])\n",
        "\n",
        "# 5. Output\n",
        "print(\"\\nCalculation Complete! Preview:\")\n",
        "print(summary.head())\n",
        "\n",
        "# Save to CSV\n",
        "summary.to_csv('Event_Summary_CN_S.csv', index=False)\n",
        "files.download('Event_Summary_CN_S.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "EJJVW4yAYmfZ",
        "outputId": "0993d535-6fdc-4f8f-a67b-d0efc555ad49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Curve Numbers...\n",
            "\n",
            "Calculation Complete! Preview:\n",
            "  Site  Event     Frozen          Start_Time            End_Time  \\\n",
            "0  MC1      1     frozen 2017-02-15 07:14:00 2017-02-18 17:16:00   \n",
            "1  MC1      2  nonfrozen 2017-04-14 23:00:00 2017-04-15 08:23:00   \n",
            "2  MC1      3  nonfrozen 2017-04-19 09:24:00 2017-04-19 20:00:00   \n",
            "3  MC1      4  nonfrozen 2017-04-30 15:00:00 2017-05-01 09:30:00   \n",
            "4  MC1      5  nonfrozen 2017-05-16 19:46:00 2017-05-18 00:10:00   \n",
            "\n",
            "   Total_Rainfall_mm  Total_Runoff_mm      S_mm        CN  \n",
            "0              0.000           1.4416    0.0000  100.0000  \n",
            "1             25.146           0.0081  120.7776   67.7735  \n",
            "2             20.828           0.0136   98.3187   72.0938  \n",
            "3             31.750           0.0391  146.6791   63.3924  \n",
            "4             31.750           0.6395  114.3631   68.9537  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6497a57a-e861-4c94-8039-f7325e463f2a\", \"Event_Summary_CN_S.csv\", 17709)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RW NORTH SITE"
      ],
      "metadata": {
        "id": "u3PaIMoikN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WinbKiWAeMtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get rainfall data"
      ],
      "metadata": {
        "id": "kznF7E6JT-_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1LZIpzGVfSbKe9J8NH3sRRE_n26yi4HeRQdapKdx43dQ' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Precipitation\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_p_n = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_p_n.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "0Ug7Q7_CT84F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_n.info()"
      ],
      "metadata": {
        "id": "r94QuXi5UFpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_n['Rainfall_mm'] = pd.to_numeric(df_p_n['Rainfall_mm'], errors='coerce') #transformar object em float\n",
        "df_p_n['Rainfall_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_p_n['Date'] = pd.to_datetime(df_p_n['Date'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "id": "44rNM3eZUHV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_n.info()"
      ],
      "metadata": {
        "id": "B6PG5zevWgCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get runoff data"
      ],
      "metadata": {
        "id": "T-Sl5MfxWx-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1LZIpzGVfSbKe9J8NH3sRRE_n26yi4HeRQdapKdx43dQ' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Runoff\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_q_n = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_q_n.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "zpGSggAoWrtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_n.info()"
      ],
      "metadata": {
        "id": "gW1JRsl6Wrtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_n['Volume_mm'] = pd.to_numeric(df_q_n['Volume_mm'], errors='coerce') #transformar object em float\n",
        "df_q_n['Volume_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_q_n['Event'] = pd.to_numeric(df_q_n['Event'], errors='coerce') #transformar object em float\n",
        "df_q_n['Event'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_q_n['Date'] = pd.to_datetime(df_q_n['Date'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RMdD1bM8Wrtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_n.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7BCD01EMWrtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get start and end of each event"
      ],
      "metadata": {
        "id": "jTHcEffEXVN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events_time_n = (\n",
        "    df_q_n\n",
        "    .groupby('Event')['Date']\n",
        "    .agg(start_time='min', end_time='max')\n",
        "    .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "5JAxmL8mXXQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_time_n = events_time_n[events_time_n['Event'] != 0]"
      ],
      "metadata": {
        "id": "02luVa5AcINx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "events_time_n.to_csv('RW1S_events_start_end.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RW1S_events_start_end.csv')"
      ],
      "metadata": {
        "id": "uHhx8EZcDRF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge precipitation and runoff data on minute interval"
      ],
      "metadata": {
        "id": "YuocFC98Y3k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_index_n = pd.date_range(\n",
        "    start='2016-10-01 00:00:00',\n",
        "    end='2024-09-30 00:00:00',\n",
        "    freq='1min'\n",
        ")\n",
        "\n",
        "df_time_n = pd.DataFrame({'Date': time_index_n})"
      ],
      "metadata": {
        "id": "drWD8nvPZfU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_n = df_time_n.merge(\n",
        "    df_q_n,\n",
        "    on='Date',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "q0d2bm8UZiL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_n = df_master_n.merge(\n",
        "    df_p_n[['Date', 'Rainfall_mm']],\n",
        "    on='Date',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "YF1walrqZ1ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create columns with date and time only"
      ],
      "metadata": {
        "id": "dLOEe4Yzdpzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_n['Date1'] = df_master_n['Date'].dt.date\n",
        "df_master_n['Time'] = df_master_n['Date'].dt.time"
      ],
      "metadata": {
        "id": "uUdGwtWJdhh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separate events"
      ],
      "metadata": {
        "id": "YOt5G7KibNyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_n = df_master_n.sort_values('Date')\n",
        "events_time_n = events_time_n.sort_values('start_time')"
      ],
      "metadata": {
        "id": "yAfbyk8XbNkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Garantir datetime completo\n",
        "df_master_n['Date'] = pd.to_datetime(df_master_n['Date'])\n",
        "\n",
        "# Criar colunas auxiliares uma única vez\n",
        "df_master_n['date'] = df_master_n['Date'].dt.date\n",
        "df_master_n['is_midnight'] = (\n",
        "    (df_master_n['Date'].dt.hour == 0) &\n",
        "    (df_master_n['Date'].dt.minute == 0)\n",
        ")"
      ],
      "metadata": {
        "id": "O0_c8TYCdv6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs_events_n = []\n",
        "\n",
        "for _, ev in events_time_n.iterrows():\n",
        "\n",
        "    start = ev['start_time']\n",
        "    end = ev['end_time']\n",
        "    event_id = ev['Event']\n",
        "\n",
        "    # --- CHANGE 1: Define the lookback period ---\n",
        "    # We want rainfall from 6 hours before, but flow only from the actual start\n",
        "    start_rain = start - pd.Timedelta(hours=6)\n",
        "\n",
        "    # 1. Intervalo do evento (Updated to use start_rain)\n",
        "    mask_event = (\n",
        "        (df_master_n['Date'] >= start_rain) &\n",
        "        (df_master_n['Date'] <= end)\n",
        "    )\n",
        "    # Use .copy() to avoid SettingWithCopy warnings when we modify columns later\n",
        "    df_ev_n = df_master_n.loc[mask_event].copy()\n",
        "\n",
        "    # --- CHANGE 2: Zero out Flow Volume before the event actually starts ---\n",
        "    # We only want the RAIN from the previous 6 hours, not the flow.\n",
        "    # If we don't do this, we might add baseflow to the Event Volume.\n",
        "    df_ev_n.loc[df_ev_n['Date'] < start, 'Volume_mm'] = 0\n",
        "\n",
        "    # 2. Dias cobertos pelo evento (Updated to check from start_rain)\n",
        "    # This ensures we catch the midnight row of the previous day if\n",
        "    # the 6-hour lookback crosses midnight.\n",
        "    days_event_n = pd.date_range(\n",
        "        start_rain.date(),\n",
        "        end.date(),\n",
        "        freq='D'\n",
        "    ).date\n",
        "\n",
        "    # 3. Chuva diária às 00:00 (Logic remains same, but uses updated days list)\n",
        "    mask_daily = (\n",
        "        df_master_n['date'].isin(days_event_n) &\n",
        "        df_master_n['is_midnight'] &\n",
        "        df_master_n['Rainfall_mm'].notna()\n",
        "    )\n",
        "    df_daily_n = df_master_n.loc[mask_daily]\n",
        "\n",
        "    # 4. Unir e ordenar\n",
        "    df_event_full_n = (\n",
        "        pd.concat([df_ev_n, df_daily_n])\n",
        "        .drop_duplicates(subset='Date')\n",
        "        .sort_values('Date')\n",
        "    )\n",
        "\n",
        "    df_event_full_n = df_event_full_n.copy()\n",
        "    df_event_full_n['Event'] = event_id\n",
        "\n",
        "    dfs_events_n.append(df_event_full_n)"
      ],
      "metadata": {
        "id": "Lfv7dMJ4f03F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_events_ext_n = pd.concat(dfs_events_n, ignore_index=True)"
      ],
      "metadata": {
        "id": "Msa0-Lf8gLYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "df_master_events_ext_n.to_csv('RWN1_events_extended.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RWN1_events_extended.csv')"
      ],
      "metadata": {
        "id": "xaW9FRvtcfTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sum event variables"
      ],
      "metadata": {
        "id": "zkKOcOkpiaX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_n = (\n",
        "    df_master_events_ext_n\n",
        "    .groupby('Event', as_index=False)\n",
        "    .agg(\n",
        "        Volume_event_mm=('Volume_mm', 'sum'),\n",
        "        Rainfall_event_mm=('Rainfall_mm', 'sum')\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "0YkQpcbYib38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_n[\n",
        "    (event_sums['Rainfall_event_mm'] == 0) |\n",
        "    (event_sums['Volume_event_mm'] == 0)\n",
        "]"
      ],
      "metadata": {
        "id": "lkTmKJ1QifkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minutes_per_event_n = (\n",
        "    df_master_events_ext_n\n",
        "    .groupby('Event')['Date']\n",
        "    .count()\n",
        "    .reset_index(name='n_minutes')\n",
        ")\n",
        "\n",
        "event_sums_n = event_sums_n.merge(minutes_per_event_n, on='Event')"
      ],
      "metadata": {
        "id": "CgDbl40wio9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_n"
      ],
      "metadata": {
        "id": "aQe3ZzEfirzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "event_sums_n.to_csv('RWN1_sums.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RWN1_sums.csv')"
      ],
      "metadata": {
        "id": "89Gx6M9vi1CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RW SOUTH SITE"
      ],
      "metadata": {
        "id": "DOEKDXbKkZUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get rainfall data"
      ],
      "metadata": {
        "id": "M9uuy7_mkZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1AJzpMK6QYbVPqdcseUkIfdKCwBDHttrcKxfWti2iAec' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Precipitation\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_p_s = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_p_s.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "RXpeV09DkZUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_s.info()"
      ],
      "metadata": {
        "id": "Me_XlkzwkZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_s['Rainfall_mm'] = pd.to_numeric(df_p_s['Rainfall_mm'], errors='coerce') #transformar object em float\n",
        "df_p_s['Rainfall_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_p_s['Date'] = pd.to_datetime(df_p_s['Date'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "id": "AGT9K4EUkZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_s.info()"
      ],
      "metadata": {
        "id": "PFmm8uASkZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get runoff data"
      ],
      "metadata": {
        "id": "Iqq0yxD6kZUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1AJzpMK6QYbVPqdcseUkIfdKCwBDHttrcKxfWti2iAec' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Runoff\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_q_s = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_q_s.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "84t4R9ELkZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_s.info()"
      ],
      "metadata": {
        "id": "Fm-QJ4qukZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_s['Volume_mm'] = pd.to_numeric(df_q_s['Volume_mm'], errors='coerce') #transformar object em float\n",
        "df_q_s['Volume_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_q_s['Event'] = pd.to_numeric(df_q_s['Event'], errors='coerce') #transformar object em float\n",
        "df_q_s['Event'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_q_s['Date'] = pd.to_datetime(df_q_s['Date'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5Og2g987kZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_s.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IwSz421TkZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get start and end of each event"
      ],
      "metadata": {
        "id": "xqcd9kqdkZUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events_time_s = (\n",
        "    df_q_s\n",
        "    .groupby('Event')['Date']\n",
        "    .agg(start_time='min', end_time='max')\n",
        "    .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "chinKJwakZUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_time_s = events_time_s[events_time_s['Event'] != 0]"
      ],
      "metadata": {
        "id": "zK1hCm3skZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "events_time_s.to_csv('RW1S_events_start_end.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RW1S_events_start_end.csv')"
      ],
      "metadata": {
        "id": "vcGxuzQeDeja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge precipitation and runoff data on minute interval"
      ],
      "metadata": {
        "id": "jqPjpH2skZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_index_s = pd.date_range(\n",
        "    start='2016-10-01 00:00:00',\n",
        "    end='2024-09-30 00:00:00',\n",
        "    freq='1min'\n",
        ")\n",
        "\n",
        "df_time_s = pd.DataFrame({'Date': time_index_s})"
      ],
      "metadata": {
        "id": "v8TpcPwNkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_s = df_time_s.merge(\n",
        "    df_q_s,\n",
        "    on='Date',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "nXGxO2HzkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_s = df_master_s.merge(\n",
        "    df_p_s[['Date', 'Rainfall_mm']],\n",
        "    on='Date',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "jytgsTSvkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create columns with date and time only"
      ],
      "metadata": {
        "id": "fXj2sjFekZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_s['Date1'] = df_master_s['Date'].dt.date\n",
        "df_master_s['Time'] = df_master_s['Date'].dt.time"
      ],
      "metadata": {
        "id": "kkinZowIkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separate events"
      ],
      "metadata": {
        "id": "QByhWA5ikZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_s = df_master_s.sort_values('Date')\n",
        "events_time_s = events_time_s.sort_values('start_time')"
      ],
      "metadata": {
        "id": "R02gNrXrkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Garantir datetime completo\n",
        "df_master_s['Date'] = pd.to_datetime(df_master_s['Date'])\n",
        "\n",
        "# Criar colunas auxiliares uma única vez\n",
        "df_master_s['date'] = df_master_s['Date'].dt.date\n",
        "df_master_s['is_midnight'] = (\n",
        "    (df_master_s['Date'].dt.hour == 0) &\n",
        "    (df_master_s['Date'].dt.minute == 0)\n",
        ")"
      ],
      "metadata": {
        "id": "DEKEcBGQkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs_events_s = []\n",
        "\n",
        "for _, ev in events_time_s.iterrows():\n",
        "\n",
        "    start = ev['start_time']\n",
        "    end = ev['end_time']\n",
        "    event_id = ev['Event']\n",
        "\n",
        "    # --- CHANGE 1: Define the lookback period ---\n",
        "    # We want rainfall from 6 hours before, but flow only from the actual start\n",
        "    start_rain = start - pd.Timedelta(hours=6)\n",
        "\n",
        "    # 1. Intervalo do evento (Updated to use start_rain)\n",
        "    mask_event = (\n",
        "        (df_master_s['Date'] >= start_rain) &\n",
        "        (df_master_s['Date'] <= end)\n",
        "    )\n",
        "    # Use .copy() to avoid SettingWithCopy warnings when we modify columns later\n",
        "    df_ev_s = df_master_s.loc[mask_event].copy()\n",
        "\n",
        "    # --- CHANGE 2: Zero out Flow Volume before the event actually starts ---\n",
        "    # We only want the RAIN from the previous 6 hours, not the flow.\n",
        "    # If we don't do this, we might add baseflow to the Event Volume.\n",
        "    df_ev_s.loc[df_ev_s['Date'] < start, 'Volume_mm'] = 0\n",
        "\n",
        "    # 2. Dias cobertos pelo evento (Updated to check from start_rain)\n",
        "    # This ensures we catch the midnight row of the previous day if\n",
        "    # the 6-hour lookback crosses midnight.\n",
        "    days_event_s = pd.date_range(\n",
        "        start_rain.date(),\n",
        "        end.date(),\n",
        "        freq='D'\n",
        "    ).date\n",
        "\n",
        "    # 3. Chuva diária às 00:00 (Logic remains same, but uses updated days list)\n",
        "    mask_daily = (\n",
        "        df_master_s['date'].isin(days_event_s) &\n",
        "        df_master_s['is_midnight'] &\n",
        "        df_master_s['Rainfall_mm'].notna()\n",
        "    )\n",
        "    df_daily_s = df_master_s.loc[mask_daily]\n",
        "\n",
        "    # 4. Unir e ordenar\n",
        "    df_event_full_s = (\n",
        "        pd.concat([df_ev_s, df_daily_s])\n",
        "        .drop_duplicates(subset='Date')\n",
        "        .sort_values('Date')\n",
        "    )\n",
        "\n",
        "    df_event_full_s = df_event_full_s.copy()\n",
        "    df_event_full_s['Event'] = event_id\n",
        "\n",
        "    dfs_events_s.append(df_event_full_s)"
      ],
      "metadata": {
        "id": "zx0_b6dlkZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master_events_ext_s = pd.concat(dfs_events_s, ignore_index=True)"
      ],
      "metadata": {
        "id": "UDfDOa2okZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "df_master_events_ext_s.to_csv('RW1S_events_extended.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RW1S_events_extended.csv')"
      ],
      "metadata": {
        "id": "qx5q9GI6kZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sum event variables"
      ],
      "metadata": {
        "id": "PmAQ6IZwkZUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_s = (\n",
        "    df_master_events_ext_s\n",
        "    .groupby('Event', as_index=False)\n",
        "    .agg(\n",
        "        Volume_event_mm=('Volume_mm', 'sum'),\n",
        "        Rainfall_event_mm=('Rainfall_mm', 'sum')\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "i5Wy80IRkZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_s[\n",
        "    (event_sums_s['Rainfall_event_mm'] == 0) |\n",
        "    (event_sums_s['Volume_event_mm'] == 0)\n",
        "]"
      ],
      "metadata": {
        "id": "8rf3SxLukZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minutes_per_event_s = (\n",
        "    df_master_events_ext_s\n",
        "    .groupby('Event')['Date']\n",
        "    .count()\n",
        "    .reset_index(name='n_minutes')\n",
        ")\n",
        "\n",
        "event_sums_s = event_sums_s.merge(minutes_per_event_s, on='Event')"
      ],
      "metadata": {
        "id": "_7DnlNeYkZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_sums_s"
      ],
      "metadata": {
        "id": "o9zqWa4LkZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "event_sums_s.to_csv('RW1S_sums.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('RW1S_sums.csv')"
      ],
      "metadata": {
        "id": "52sRrunXkZUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#MC Site"
      ],
      "metadata": {
        "id": "0j8ptYoH7oGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get rainfall data"
      ],
      "metadata": {
        "id": "LtFTsnxh7tj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1gXo0bOYZxrkakpnB8twwDeczVk4Qg5xMV9HtVop1dP4' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Precipitation\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_p_mc = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_p_mc.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "vTbdkrf87tkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_mc.info()"
      ],
      "metadata": {
        "id": "XDb0LWfB7tkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_mc['Rainfall_mm'] = pd.to_numeric(df_p_mc['Rainfall_mm'], errors='coerce') #transformar object em float\n",
        "df_p_mc['Rainfall_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_p_mc['Date'] = pd.to_datetime(df_p_mc['Date'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "id": "mk66_0Br7tkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p_mc.info()"
      ],
      "metadata": {
        "id": "FVqLq-CB7tkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get start and end of each event"
      ],
      "metadata": {
        "id": "7AbfeQpY7tkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id = '1gXo0bOYZxrkakpnB8twwDeczVk4Qg5xMV9HtVop1dP4' #id da planilha do google sheets - retirado do url ###Planilha vazao_serie\n",
        "spreadsheet = gc.open_by_key(id) #para abrir spreadsheet pela chave\n",
        "sheet = spreadsheet.worksheet(\"Runoff\") #nome da planilha a ser trabalhada\n",
        "rows = sheet.get_all_values()\n",
        "df_q_mc = pd.DataFrame(rows[1:]) #transforma em DataFrame, a partir da linha 1 (linha 0 = cabeçalho)\n",
        "df_q_mc.columns = rows[0] #para transformar a primeira linha"
      ],
      "metadata": {
        "id": "xRLnsQWY7tkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_mc.info()"
      ],
      "metadata": {
        "id": "csR5kEBX7tkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_mc['Volume_mm'] = pd.to_numeric(df_q_mc['Volume_mm'], errors='coerce') #transformar object em float\n",
        "df_q_mc['Volume_mm'].fillna(0, inplace=True) #preenche NA com 0\n",
        "df_q_mc['Start_event'] = pd.to_datetime(df_q_mc['Start_event'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime\n",
        "df_q_mc['End_event'] = pd.to_datetime(df_q_mc['End_event'], format=\"%m/%d/%Y %H:%M:%S\") #converte string para datetime"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g-8iZ9Vb7tkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_mc.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iuehuSuG7tkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sum rainfall during each event"
      ],
      "metadata": {
        "id": "MqFrs5kp7tkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soma_chuva_evento(df_p_mc, inicio, fim):\n",
        "    mask = (df_p_mc['Date'] >= inicio) & (df_p_mc['Date'] <= fim)\n",
        "    return df_p_mc.loc[mask, 'Rainfall_mm'].sum()"
      ],
      "metadata": {
        "id": "u52MraUz7tkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply function to each event"
      ],
      "metadata": {
        "id": "nMMDQUwR-P33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_q_mc['chuva_total_mm'] = df_q_mc.apply(\n",
        "    lambda row: soma_chuva_evento(\n",
        "        df_p_mc,\n",
        "        row['Start_event'],\n",
        "        row['End_event']\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "PVnBJ0Bk7tkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save\n",
        "df_q_mc.to_csv('MC1_sums.csv', index=True)\n",
        "\n",
        "#Download\n",
        "files.download('MC1_sums.csv')"
      ],
      "metadata": {
        "id": "8huLlEC67tkF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}